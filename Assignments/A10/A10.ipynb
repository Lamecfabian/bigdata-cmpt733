{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 10 - NLP using Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment you will get to work with recurrent network architectures with application to language processing tasks and observe behaviour of the learning using tensorboard visualization.\n",
    "\n",
    "You'll learn to use\n",
    "\n",
    " * word embeddings\n",
    " * LSTMs\n",
    " * tensorboard visualization\n",
    " * optionally, but easy to try, state-of-the-art transformer model\n",
    "\n",
    "While the notebook is heavy with code, the actual **TODO**s for you are lightweight and easy to find. Use the lab machines and provided environment to get started and finish quickly.\n",
    "The main intention of this exercise is to provide you with entry points to approach common NLP tasks with simple and elaborate methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use the deep learning environment in the lab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the same kind of preparation as in [Assignment 6](../A6/A6.ipynb) we are going to use **[pytorch](http://pytorch.org)** for the deep learning aspects of the assignment.\n",
    "\n",
    "There is a pytorch setup in the big data under the globally available anaconda installation.\n",
    "However, it is recommended that you use the custom **gt** conda environment that contains all python package dependencies that are relevant for this assignment (and also tensorflow, etc.)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You could load it directly\n",
    "```\n",
    "source activate /usr/shared/CMPT/big-data/condaenv/gt\n",
    "```\n",
    "Once activated, you couls also add it as a user kernel to your jupyter installation\n",
    "```\n",
    "python -m ipykernel install --user --name=\"py-gt\"\n",
    "```\n",
    "and then choose it as kernel when running this notebook.\n",
    "To reproduce this environment on your own system, you could use `conda env export > environment.yml` and then use `mamba env update --prefix wherever_you_want_to_create_yours -f environment.yml` to make your own instance of this environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "DEVICE = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "# location of \"GoogleNews-vectors-negative300.bin.gz\", only required if word2vec embedding is chosen\n",
    "from pathlib import Path\n",
    "bdenv_loc = Path('/usr/shared/CMPT/big-data')\n",
    "bdata = bdenv_loc / 'data'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1: Explore Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word embeddings are mappings between words and multi-dimensional vectors, where the difference between two word vectors has some relationship with the meaning of the corresponding words, i.e. words that are similar in meaning are mapped closely together (ideally). This part of the assignment should enable you to\n",
    "\n",
    "* Load a pretrained word embedding\n",
    "* Perform basic operations, such as distance queries and evaluate simple analogies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "# Load Google's pre-trained Word2Vec model, trained on news articles\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format(\n",
    "    bdata / 'GoogleNews-vectors-negative300.bin.gz', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300,)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read up about the word2vec API in gensim and\n",
    "# obtain a vector representation for a word of your choice\n",
    "\n",
    "# TODO ...\n",
    "\n",
    "# to confirm that this worked, print out the number of elements of the vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine the 10 words that are closest in the embedding to the word vector your produced above\n",
    "\n",
    "# TODO ...\n",
    "\n",
    "# are the nearest neighbours similar in meaning?\n",
    "# try different seed words, until you find one whose neighbourhood looks OK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using a combination of positive and negative words, find out which word is most\n",
    "# similar to woman + king - man\n",
    "\n",
    "# TODO ...\n",
    "\n",
    "# note, gensim's API allows you to combine positive and negative words without having to obtain their vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you may find that the results of most word analogy combinations don't work as well as we'd hope.\n",
    "# however, explore a bit and find two more cases where the output of your word vector algebra makes sense.\n",
    "\n",
    "# TODO ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rerun at least one of your above word embedding examples using a different embedding, instead of word2vec, i.e. this version of GLOVE\n",
    "\n",
    "#import gensim.downloader as api\n",
    "#word_vectors = api.load(\"glove-wiki-gigaword-100\")\n",
    "\n",
    "# TODO ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2: Sequence modeling with RNNs or transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this task you will get to use a learning and a rule-based model of text sentiment analysis. To keep things simple, you will receive almost all the code and are just left with the task to tune the given algorithms, see the part about instrumentation below.\n",
    "Look for *TODO* to find places where your input is required."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SST-2 Binary text classification with XLM-RoBERTa model and LSTMs\n",
    "\n",
    "The XLM-RoBERTa related portions of this notebook are from [a tutorial](https://pytorch.org/text/main/tutorials/sst2_classification_non_distributed.html) authored by `Parmeet Bhatia <parmeetbhatia@fb.com>`\n",
    "\n",
    "Adaptation of the modern torchtext pipeline to also allow switching to recurrent model with different pre-trained word embeddings by `Steven Bergner <sbergner@sfu.ca>`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The steps below demonstrate how to train a text classifier on SST-2 binary dataset using a pre-trained XLM-RoBERTa (XLM-R) model. Customizations to switch parts of the pipeline to different models are also enabled.\n",
    "\n",
    "We will show how to use torchtext library to:\n",
    "\n",
    "1. build text pre-processing pipeline for XLM-R model\n",
    "2. read SST-2 dataset and transform it using text and label transformation\n",
    "3. instantiate a classification model using pre-trained XLM-R encoder\n",
    "4. change pipeline components to swap out any part of the data and model pipeline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Transformation\n",
    "\n",
    "Models like XLM-R cannot work directly with raw text. The first step in training\n",
    "these models is to transform input text into tensor (numerical) form such that it\n",
    "can then be processed by models to make predictions. A standard way to process text is:\n",
    "\n",
    "1. Tokenize text\n",
    "2. Convert tokens into (integer) IDs\n",
    "3. Add any special tokens IDs\n",
    "\n",
    "XLM-R uses sentencepiece model for text tokenization. Below, we use pre-trained sentence piece\n",
    "model along with corresponding vocabulary to build text pre-processing pipeline using torchtext's transforms.\n",
    "The transforms are pipelined using :py:func:`torchtext.transforms.Sequential` which is similar to :py:func:`torch.nn.Sequential`\n",
    "but is torchscriptable. Note that the transforms support both batched and non-batched text inputs i.e, one\n",
    "can either pass a single sentence or list of sentences.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Caution: If you want to learn more about torchtext, be careful to **not** read the docs at:\n",
    "https://torchtext.readthedocs.io/en/latest/\n",
    "They claim to be \"latest\", but are of version 0.4.0\n",
    "\n",
    "Instead, find **current docs** here: https://pytorch.org/text/stable/index.html\n",
    "or simply keep reading, as this tutorial shows how to use the recent version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import torchtext.transforms as T\n",
    "from torch.hub import load_state_dict_from_url\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "padding_idx = 1\n",
    "bos_idx = 0\n",
    "eos_idx = 2\n",
    "max_seq_len = 256\n",
    "xlmr_vocab_path = r\"https://download.pytorch.org/models/text/xlmr.vocab.pt\"\n",
    "xlmr_spm_model_path = r\"https://download.pytorch.org/models/text/xlmr.sentencepiece.bpe.model\"\n",
    "\n",
    "text_transform = T.Sequential(\n",
    "    T.SentencePieceTokenizer(xlmr_spm_model_path),\n",
    "    T.VocabTransform(load_state_dict_from_url(xlmr_vocab_path)),\n",
    "    T.Truncate(max_seq_len - 2),\n",
    "    T.AddToken(token=bos_idx, begin=True),\n",
    "    T.AddToken(token=eos_idx, begin=False),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain the vocabulary of the data pipeline, so that we can convert word <--> word_index\n",
    "# allowing us to plug in different word embeddings\n",
    "vocab = text_transform[1].vocab.vocab\n",
    "word_to_idx = vocab.get_stoi()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to the transformer model, we also create an LSTM based model for text classification.\n",
    "\n",
    "Change the parameters below to switch between models and make adjustments to the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# TODO make adjustments here to achieve acceptable training performance with LSTMs\n",
    "# Also, try out the Roberta model for comparison\n",
    "\n",
    "EPOCHS = 8\n",
    "USE_GPU = torch.cuda.is_available()\n",
    "DROPOUT = .1\n",
    "timestamp = str(int(time.time()))\n",
    "best_dev_acc = 0.0\n",
    "\n",
    "do_use_roberta_model = False\n",
    "if do_use_roberta_model:\n",
    "    LEARNING_RATE = 1e-5\n",
    "    EPOCHS = 1\n",
    "    BATCH_SIZE = 16\n",
    "    EMBEDDING_TYPE = 'built-in'\n",
    "else:\n",
    "    #EMBEDDING_TYPE = 'word2vec'\n",
    "    EMBEDDING_TYPE = 'glove'\n",
    "    #EMBEDDING_TYPE = 'glovefull'\n",
    "    EMBEDDING_DIM = 300\n",
    "    HIDDEN_DIM = 50\n",
    "    BATCH_SIZE = 128\n",
    "    USE_BILSTM = True\n",
    "    LEARNING_RATE = 1e-5\n",
    "    do_freeze_embedding = True\n",
    "    do_use_roberta_classifier = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maybe_gpu(v):\n",
    "    return v.cuda() if USE_GPU else v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as nnF\n",
    "\n",
    "class LSTMSentiment(nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size, label_size,\n",
    "                 use_gpu, batch_size, dropout=0.5, bidirectional=False, classifier_head=None):\n",
    "        \"\"\"Prepare individual layers\"\"\"\n",
    "        super(LSTMSentiment, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.use_gpu = use_gpu\n",
    "        self.batch_size = batch_size\n",
    "        self.dropout = dropout\n",
    "        self.num_directions = 2 if bidirectional else 1\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(input_size=embedding_dim, hidden_size=hidden_dim, bidirectional=bidirectional)\n",
    "        self.hidden2label = nn.Linear(hidden_dim*self.num_directions, label_size)\n",
    "        self.hidden = self.init_hidden()\n",
    "        self.classifier_head = classifier_head\n",
    "\n",
    "    def init_hidden(self, batch_size=None):\n",
    "        \"\"\"Choose appropriate size and type of hidden layer\"\"\"\n",
    "        if not batch_size:\n",
    "            batch_size = self.batch_size\n",
    "        #what = torch.randn\n",
    "        what = torch.zeros\n",
    "        # first is the hidden h\n",
    "        # second is the cell c\n",
    "        return (maybe_gpu(Variable(what(self.num_directions, batch_size, self.hidden_dim))),\n",
    "                maybe_gpu(Variable(what(self.num_directions, batch_size, self.hidden_dim))))\n",
    "\n",
    "    def classify(self, features):\n",
    "        y = self.hidden2label(features)\n",
    "        log_probs = nnF.log_softmax(y, dim=1)\n",
    "        return log_probs\n",
    "\n",
    "    def forward(self, sentence):\n",
    "        \"\"\"Use the layers of this model to propagate input and return class log probabilities\"\"\"\n",
    "        if self.use_gpu:\n",
    "            sentence = sentence.cuda()\n",
    "        x = self.embeddings(sentence).permute(1,0,2)\n",
    "        batch_size = x.shape[1]\n",
    "        self.hidden = self.init_hidden(batch_size=batch_size)\n",
    "        lstm_out, self.hidden = self.lstm(x, self.hidden)\n",
    "        features = lstm_out[-1]\n",
    "        if self.classifier_head:\n",
    "            #unsqueeze: introduce dummy second dimension, so that classifier_head can drop it\n",
    "            return self.classifier_head(torch.unsqueeze(features, 1))\n",
    "        else:\n",
    "            return self.classify(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose and load a word embedding that provides the feature input to the RNN/LSTM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'glove' == EMBEDDING_TYPE:\n",
    "    from torchtext.vocab import GloVe\n",
    "    glove_vectors = GloVe(name=\"6B\")\n",
    "    EMBEDDING_DIM = glove_vectors.vectors.shape[1]\n",
    "    use_embedding_directly = False\n",
    "    if use_embedding_directly:\n",
    "        pretrained_embeddings = maybe_gpu(glove_vectors.vectors)\n",
    "    else:\n",
    "        \n",
    "        pretrained_embeddings = np.random.uniform(-0.25, 0.25, (len(vocab), EMBEDDING_DIM)).astype('f')\n",
    "        pretrained_embeddings[0] = 0\n",
    "        for word, wi in glove_vectors.stoi.items():\n",
    "            try:\n",
    "                pretrained_embeddings[word_to_idx[word]-1] = glove_vectors.__getitem__(word)\n",
    "            except KeyError:\n",
    "                pass\n",
    "        pretrained_embeddings = maybe_gpu(torch.from_numpy(pretrained_embeddings))\n",
    "elif 'glovefull' == EMBEDDING_TYPE:\n",
    "    from torchtext.vocab import GloVe\n",
    "    glove_vectors = GloVe(cache=\"/usr/shared/CMPT/big-data/dot_torch_shared/.vector_cache/\")\n",
    "    # set freeze to false if you want them to be trainable\n",
    "    pretrained_embeddings = maybe_gpu(glove_vectors.vectors)\n",
    "    #my_embeddings = nn.Embedding.from_pretrained(pretrained_embeddings, freeze=True)\n",
    "elif 'word2vec' == EMBEDDING_TYPE:\n",
    "    pretrained_embeddings = np.random.uniform(-0.25, 0.25, (len(vocab), EMBEDDING_DIM)).astype('f')\n",
    "    pretrained_embeddings[0] = 0\n",
    "    try:\n",
    "        word2vec\n",
    "    except:\n",
    "        print('Load word embeddings...')\n",
    "        import gensim\n",
    "        word2vec = gensim.models.KeyedVectors.load_word2vec_format(\n",
    "                         bdata / 'GoogleNews-vectors-negative300.bin.gz', binary=True)\n",
    "        EMBEDDING_DIM = 300\n",
    "    for word, wi in word2vec.key_to_index.items():\n",
    "        try:\n",
    "            pretrained_embeddings[word_to_idx[word]-1] = word2vec.vectors[wi]\n",
    "        except KeyError:\n",
    "            pass\n",
    "    # text_field.vocab.load_vectors(wv_type='', wv_dim=300)\n",
    "    pretrained_embeddings = maybe_gpu(torch.from_numpy(pretrained_embeddings))\n",
    "else:\n",
    "    if not do_use_roberta_model:\n",
    "        print('Unknown embedding type {}'.format(EMBEDDING_TYPE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model preparation LSTM\n",
    "Initialize the RNN model, if the above configuration is set to use it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 2\n",
    "\n",
    "if not do_use_roberta_model:\n",
    "    lstm_model = LSTMSentiment(embedding_dim=EMBEDDING_DIM, hidden_dim=HIDDEN_DIM,\n",
    "                                vocab_size=len(vocab), label_size=num_classes,\\\n",
    "                                use_gpu=USE_GPU, batch_size=BATCH_SIZE, dropout=DROPOUT, bidirectional=USE_BILSTM)\n",
    "    lstm_model.embeddings = nn.Embedding.from_pretrained(pretrained_embeddings, freeze=do_freeze_embedding)\n",
    "    model = lstm_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternately we can also use transform shipped with pre-trained model that does all of the above out-of-the-box\n",
    "\n",
    "::\n",
    "\n",
    "  text_transform = XLMR_BASE_ENCODER.transform()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "torchtext provides several standard NLP datasets. For complete list, refer to documentation\n",
    "at https://pytorch.org/text/stable/datasets.html. These datasets are build using composable torchdata\n",
    "datapipes and hence support standard flow-control and mapping/transformation using user defined functions\n",
    "and transforms. Below, we demonstrate how to use text and label processing transforms to pre-process the\n",
    "SST-2 dataset.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from torchtext.datasets import SST2\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "batch_size = BATCH_SIZE\n",
    "\n",
    "train_datapipe = SST2(split=\"train\")\n",
    "dev_datapipe = SST2(split=\"dev\")\n",
    "\n",
    "# Transform the raw dataset using non-batched API (i.e apply transformation line by line)\n",
    "train_datapipe = train_datapipe.map(lambda x: (text_transform(x[0]), x[1]))\n",
    "train_datapipe = train_datapipe.batch(batch_size)\n",
    "train_datapipe = train_datapipe.rows2columnar([\"token_ids\", \"target\"])\n",
    "train_dataloader = DataLoader(train_datapipe, batch_size=None)\n",
    "\n",
    "dev_datapipe = dev_datapipe.map(lambda x: (text_transform(x[0]), x[1]))\n",
    "dev_datapipe = dev_datapipe.batch(batch_size)\n",
    "dev_datapipe = dev_datapipe.rows2columnar([\"token_ids\", \"target\"])\n",
    "dev_dataloader = DataLoader(dev_datapipe, batch_size=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # Alternately we can also use batched API\n",
    "# train_datapipe = train_datapipe.batch(batch_size).rows2columnar([\"text\", \"label\"])\n",
    "# train_datapipe = train_datapipe.map(lambda x: {\"token_ids\": text_transform(x[\"text\"]), \"target\": label_transform(x[\"label\"])})\n",
    "# dev_datapipe = dev_datapipe.batch(batch_size).rows2columnar([\"text\", \"label\"])\n",
    "# dev_datapipe = dev_datapipe.map(lambda x: {\"token_ids\": text_transform(x[\"text\"]), \"target\": label_transform(x[\"label\"])})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model preparation - RoBERTa\n",
    "\n",
    "torchtext provides SOTA pre-trained models that can be used to fine-tune on downstream NLP tasks.\n",
    "Below we use pre-trained XLM-R encoder with standard base architecture and attach a classifier head to fine-tune it\n",
    "on SST-2 binary classification task. We shall use standard Classifier head from the library, but users can define\n",
    "their own appropriate task head and attach it to the pre-trained encoder. For additional details on available pre-trained models,\n",
    "please refer to documentation at https://pytorch.org/text/main/models.html\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_classes = 2\n",
    "\n",
    "from torchtext.models import RobertaClassificationHead, XLMR_BASE_ENCODER\n",
    "\n",
    "if do_use_roberta_model:\n",
    "    input_dim = 768\n",
    "    classifier_head = RobertaClassificationHead(num_classes=num_classes, input_dim=input_dim)\n",
    "    model = XLMR_BASE_ENCODER.get_model(head=classifier_head)\n",
    "else:\n",
    "    model = lstm_model\n",
    "    if do_use_roberta_classifier:\n",
    "        feature_dim = model.hidden_dim + (USE_BILSTM * model.hidden_dim)\n",
    "        classifier_head = RobertaClassificationHead(num_classes=num_classes, input_dim=feature_dim)\n",
    "        model.classifier_head = classifier_head\n",
    "\n",
    "model.to(DEVICE);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training methods\n",
    "\n",
    "Let's now define the standard optimizer and training criteria as well as some helper functions\n",
    "for training and evaluation. The methods below work for either choice of model.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import torchtext.functional as F\n",
    "from torch.optim import AdamW\n",
    "\n",
    "learning_rate = LEARNING_RATE\n",
    "optim = AdamW(model.parameters(), lr=learning_rate)\n",
    "criteria = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "def train_step(input, target):\n",
    "    model.train()\n",
    "    output = model(input)\n",
    "    loss = criteria(output, target)\n",
    "    optim.zero_grad()\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "\n",
    "\n",
    "def eval_step(input, target):\n",
    "    output = model(input)\n",
    "    loss = criteria(output, target).item()\n",
    "    return float(loss), (output.argmax(1) == target).type(torch.float).sum().item()\n",
    "\n",
    "\n",
    "def evaluate():\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "    counter = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in dev_dataloader:\n",
    "            input = F.to_tensor(batch[\"token_ids\"], padding_value=padding_idx).to(DEVICE)\n",
    "            target = torch.tensor(batch[\"target\"]).to(DEVICE)\n",
    "            loss, predictions = eval_step(input, target)\n",
    "            total_loss += loss\n",
    "            correct_predictions += predictions\n",
    "            total_predictions += len(target)\n",
    "            counter += 1\n",
    "\n",
    "    return total_loss / counter, correct_predictions / total_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The actual task (B1): Tensorboard instrumentation\n",
    "\n",
    "To get you to work with the some of the basic tools that enable development and tuning of deep learning architectures, we would like you to use Tensorboard.\n",
    "\n",
    "1. read up on how to instrument your code for profiling and visualization in [tensorboard](https://www.tensorflow.org/programmers_guide/summaries_and_tensorboard), e.g. [at this tutorial](https://pytorch.org/tutorials/recipes/recipes/tensorboard_with_pytorch.html)\n",
    "1. [partly done] use the tensorboard `SummaryWriter` to keep track of training loss for each epoch, writing to a local `runs` folder (which is the default)\n",
    "1. launch tensorboard and inspect the log folder, i.e. run `tensorboard --logdir runs` from the assignment folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "out_dir = os.path.abspath(os.path.join(os.path.curdir, \"runs\", timestamp))\n",
    "\n",
    "writer = SummaryWriter(comment='-{}lstm-em{}{}-hid{}-do{}-bs{}-lr{}'\n",
    "                                .format('BI' if USE_BILSTM else '',\n",
    "                                        EMBEDDING_TYPE, EMBEDDING_DIM,\n",
    "                                        HIDDEN_DIM,\n",
    "                                        DROPOUT, BATCH_SIZE, LEARNING_RATE))\n",
    "print(\"Writing to {}\\n\".format(out_dir))\n",
    "if not os.path.exists(out_dir):\n",
    "    os.makedirs(out_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train\n",
    "\n",
    "Now we have all the ingredients to train our classification model. Note that we are able to directly iterate\n",
    "on our dataset object without using DataLoader. Our pre-process dataset  shall yield batches of data already,\n",
    "thanks to the batching datapipe we have applied. For distributed training, we would need to use DataLoader to\n",
    "take care of data-sharding.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "527it [00:08, 59.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = [0], loss = [0.6256523643221173], accuracy = [0.7924311926605505]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "527it [00:08, 59.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = [1], loss = [0.6527362380708966], accuracy = [0.7889908256880734]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "527it [00:08, 58.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = [2], loss = [0.686433596270425], accuracy = [0.7924311926605505]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "527it [00:08, 59.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = [3], loss = [0.7410882030214582], accuracy = [0.7855504587155964]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "527it [00:08, 60.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = [4], loss = [0.7260513135365078], accuracy = [0.7947247706422018]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "527it [00:09, 58.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = [5], loss = [0.759310245513916], accuracy = [0.7844036697247706]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "527it [00:08, 58.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = [6], loss = [0.770913941519601], accuracy = [0.7786697247706422]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "527it [00:08, 58.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = [7], loss = [0.7808546338762555], accuracy = [0.783256880733945]\n"
     ]
    }
   ],
   "source": [
    "num_epochs = EPOCHS\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "torch.autograd.set_detect_anomaly(False)\n",
    "\n",
    "trial = 0 # increment this if you manually decide to add more epochs to the current training\n",
    "for e in range(EPOCHS*trial,EPOCHS*(trial+1)):\n",
    "    for batch in tqdm(train_dataloader):\n",
    "        input = F.to_tensor(batch[\"token_ids\"], padding_value=padding_idx).to(DEVICE)\n",
    "        target = torch.tensor(batch[\"target\"]).to(DEVICE)\n",
    "        train_step(input, target)\n",
    "\n",
    "    loss, accuracy = evaluate()\n",
    "    print(\"Epoch = [{}], loss = [{}], accuracy = [{}]\".format(e, loss, accuracy))\n",
    "    # TODO add loss and accuracy to the tensorboard writer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO ensure that the test accuracy is visible in the saved notebook for submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task B2: Tune the model (TODO)\n",
    "\n",
    "After connecting the output of your model train and test performance with tensorboard. Change the model and training parameters above to improve the model performance. We would like to see variable plots of how validation accuracy evolves over a number of epochs for at least two different parameter choices, you can stop exploring when you exceed a model accuracy of 76%.\n",
    "\n",
    "Show a tensorboard screenshot with performance plots that combine at least 2 different tuning attempts. Store the screenshot as `tensorboard.png`. Then keep the best performing parameters set in this notebook for submission and evaluate the comparison below with your best model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison with Vader (NLTK)\n",
    "Vader is a rule-based sentiment analysis algorithm that performs quite well against more complex architectures. The test below is to see, whether LSTMs are able to beat its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get text data from torchtext dataloader\n",
    "vocab_itos = vocab.get_itos()\n",
    "text_data = []\n",
    "for ba in dev_dataloader:\n",
    "    text = (\"\".join(\n",
    "            [\"\".join(\n",
    "                vocab_itos[tid]) for tokens in ba[\"token_ids\"] \n",
    "                for tid in tokens ])\n",
    "                .replace(\"‚ñÅ\",\" \")\n",
    "                .replace(\"<s>\",\"\")\n",
    "                .split(\"</s>\"))\n",
    "    text_and_target = list(zip(text, ba[\"target\"]))\n",
    "    text_data.extend(text_and_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "#nltk.download('vader_lexicon')\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "lab_vpred = np.zeros((len(text_data), 2))\n",
    "for k, (sentence, label) in enumerate(text_data):\n",
    "    ss = sid.polarity_scores(sentence)\n",
    "    lab_vpred[k,:] = (int(ss['compound']>0), int(label))\n",
    "\n",
    "vader_acc = 1-abs(lab_vpred[:,0]-lab_vpred[:,1]).mean()\n",
    "print('vader acc: {}'.format(vader_acc))\n",
    "writer.add_scalar('Final/VaderAcc', vader_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform the model tuning and training in the previous task until you outperform the Vader algorithm by at least 7% in accuracy using the LSTM model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save [this notebook](A10.ipynb) containing all cell output and upload your submission as one `A10.ipynb` file.\n",
    "Also, include the screenshot of your tensorboard debugging session as `tensorboard.png`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
